# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import subprocess

from openrelik_worker_common.file_utils import create_output_file
from openrelik_worker_common.task_utils import create_task_result, get_input_files
from .text_formatter import heading4,heading5,bullet
from uuid import uuid4
import os
import xml.etree.ElementTree as xml_tree
import glob
from typing import List, Dict

from .app import celery

# Task name used to register and route the task to the correct queue.
TASK_NAME = "openrelik-worker-bulkextractor.tasks.bulkextractor"

# Task metadata for registration in the core system.
TASK_METADATA = {
    "display_name": "Bulkextractor",
    "description": "Runs the bulk_extractor command against a file",
    # Configuration that will be rendered as a web for in the UI, and any data entered
    # by the user will be available to the task function when executing (task_config).
}

def check_xml_attrib(xml_file, xml_key):
    """Checks if a key exists within the xml report.

        Args:
            xml_key(str): the xml key to check for.

        Returns:
            xml_hit(str): the xml value else return N/A.
        """
    xml_hit = 'N/A'
    xml_search = xml_file.find(xml_key)

    # If exists, return the text value.
    if xml_search is not None:
        xml_hit = xml_search.text
    return xml_hit

def generate_summary_report(output_dir):
    """Generate a summary report from the resulting bulk extractor run.

    Args:
        output_file_path(str): the path to the bulk extractor output.

    Returns:
        tuple: containing:
        report_test(str): The report data
        summary(str): A summary of the report (used for task status)
    """
    findings = []
    features_count = 0
    report_path = os.path.join(output_dir, 'report.xml')

    # Check if report.xml was not generated by bulk extractor.
    if not os.path.exists(report_path):
        report = 'Execution successful, but the report is not available.'
        return (report, report)

    # Parse existing XML file.
    xml_file = xml_tree.parse(report_path)

    # Place in try/except statement to continue execution when
    # an attribute is not found and NoneType is returned.
    try:
        # Retrieve summary related results.
        findings.append(heading4('Bulk Extractor Results'))
        findings.append(heading5('Run Summary'))
        findings.append(
            bullet(
                'Program: {0} - {1}'.format(
                    check_xml_attrib(xml_file,'creator/program'),
                    check_xml_attrib(xml_file,'creator/version'))))
        findings.append(
            bullet(
                'Command Line: {0}'.format(
                    check_xml_attrib(
                        xml_file,
                        'creator/execution_environment/command_line'))))
        findings.append(
            bullet(
                'Start Time: {0}'.format(
                    check_xml_attrib(
                        xml_file,
                        'creator/execution_environment/start_time'))))
        findings.append(
            bullet(
                f"Elapsed Time: {check_xml_attrib(xml_file, 'report/elapsed_seconds')}"
            ))

        # Retrieve results from each of the scanner runs and display in table
        feature_files = xml_file.find(".//feature_files")
        scanner_results = []
        if feature_files is not None:
            findings.append(heading5('Scanner Results\n'))
            for name, count in zip(xml_file.findall(".//feature_file/name"),
                                    xml_file.findall(".//feature_file/count")):
                scanner_results.append({"Name": name.text, "Count": int(count.text)})
                features_count += int(count.text)
            sorted_scanner_results = sorted(
                scanner_results, key=lambda x: x["Count"], reverse=True)
            columns = scanner_results[0].keys()
            findings.append(" | ".join(columns))
            findings.append(" | ".join(["---"] * len(columns)))
            for scanner_result in sorted_scanner_results:
                findings.append(
                    " | ".join(str(scanner_result[column]) for column in columns))
        else:
            findings.append(heading5("There are no findings to report."))
    except AttributeError as exception:
        #log.warning(
        #    f'Error parsing feature from Bulk Extractor report: {exception!s}')
        raise exception
    summary = f'{features_count} artifacts have been extracted.'
    report = '\n'.join(findings)
    return (report, summary)

def extract_non_empty_files(artifact_dir, output_path) -> List[Dict]:
    out_dir = os.path.join(artifact_dir, "output")
    os.makedirs(out_dir, exist_ok=True)
    out_files = []
    for entry in glob.glob(os.path.join(artifact_dir, '**'), recursive=True):
        if os.path.exists(entry) and not os.path.isdir(entry):
            with open(entry, "rb") as f:
                content = f.read()
                if content:
                    out_file = create_output_file(output_path, display_name=os.path.basename(entry))
                    with open(out_file.path, "wb") as out_f:
                        out_f.write(content)
                    out_files.append(out_file.to_dict())
    return out_files



@celery.task(bind=True, name=TASK_NAME, metadata=TASK_METADATA)
def command(
    self,
    pipe_result: str = None,
    input_files: list = None,
    output_path: str = None,
    workflow_id: str = None,
    task_config: dict = None,
) -> str:
    """Run bulk_extractor on input files.

    Args:
        pipe_result: Base64-encoded result from the previous Celery task, if any.
        input_files: List of input file dictionaries (unused if pipe_result exists).
        output_path: Path to the output directory.
        workflow_id: ID of the workflow.
        task_config: User configuration for the task.

    Returns:
        Base64-encoded dictionary containing task results.
    """
    input_files = get_input_files(pipe_result, input_files or [])
    output_files = []

    for input_file in input_files:
        base_command = ["bulk_extractor"]
        output_file = create_output_file(
            output_path,
            display_name="Report_{}".format(input_file.get("display_name")),
            extension="html",
            data_type="html",
        )
        artifacts_dir = os.path.join(output_path, uuid4().hex)
        base_command.extend(["-o", artifacts_dir])
        base_command_string = " ".join(base_command)
        command = base_command + [input_file.get("path")]

        # Run the command
        process  = subprocess.Popen(command)
        process.wait()
        if process.returncode == 0:
            # Execution complete, verify the results
            if os.path.exists(artifacts_dir):
                report, _ = generate_summary_report(artifacts_dir)
                with open(output_file.path, "w") as fh:
                    fh.write(report)
                output_files.append(output_file.to_dict())
                output_files.extend(extract_non_empty_files(artifacts_dir, output_path))
            else:
                print("os.path.exists({}):{} when expected True".format(artifacts_dir, os.path.exists(artifacts_dir)))
                raise
        else:
            raise

    if not output_files:
        raise RuntimeError("Error running bulk extractor, no files returned.")

    return create_task_result(
        output_files=output_files,
        workflow_id=workflow_id,
        command=base_command_string,
        meta={},
    )
